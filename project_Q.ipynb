{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "import elasticsearch.helpers\n",
    "from elasticsearch_dsl import Search\n",
    "import json\n",
    "import os\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREC run\n",
    "\n",
    "### Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = elasticsearch.Elasticsearch(host='localhost')  # in case you use Docker, the host is 'elasticsearch'\n",
    "\n",
    "def read_documents(file_name):\n",
    "    \"\"\"\n",
    "    Returns a generator of documents to be indexed by elastic, read from file_name\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r') as documents:\n",
    "        for line in documents:\n",
    "            doc_line = json.loads(line)\n",
    "            if ('index' in doc_line):\n",
    "                id = doc_line['index']['_id']\n",
    "            elif ('PMID' in doc_line):\n",
    "                doc_line['_id'] = id\n",
    "                yield doc_line\n",
    "            else:\n",
    "                raise ValueError('Woops, error in index file')\n",
    "\n",
    "def create_index(es, index_name, body={}):\n",
    "    # delete index when it already exists\n",
    "    es.indices.delete(index=index_name, ignore=[400, 404])\n",
    "    # create the index \n",
    "    es.indices.create(index=index_name, body=body)\n",
    "                \n",
    "def index_documents(es, file_name, index_name, body={}):\n",
    "    create_index(es, index_name, body)\n",
    "    # bulk index the documents from file_name\n",
    "    return elasticsearch.helpers.bulk(\n",
    "        es, \n",
    "        read_documents(file_name),\n",
    "        index=index_name,\n",
    "        chunk_size=2000,\n",
    "        request_timeout=30\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_qrels_file(qrels_file):\n",
    "    trec_relevant = dict()\n",
    "    with open(qrels_file, 'r') as qrels:\n",
    "        for line in qrels:\n",
    "            (qid, q0, doc_id, rel) = line.strip().split()\n",
    "            if qid not in trec_relevant:\n",
    "                trec_relevant[qid] = set()\n",
    "            if (rel == \"1\"):\n",
    "                trec_relevant[qid].add(doc_id)\n",
    "    return trec_relevant\n",
    "\n",
    "\n",
    "def read_run_file(run_file):  \n",
    "    trec_retrieved = dict()\n",
    "    with open(run_file, 'r') as run:\n",
    "        for line in run:\n",
    "            (qid, q0, doc_id, rank, score, tag) = line.strip().split()\n",
    "            if qid not in trec_retrieved:\n",
    "                trec_retrieved[qid] = []\n",
    "            trec_retrieved[qid].append(doc_id) \n",
    "    return trec_retrieved\n",
    "    \n",
    "\n",
    "def read_eval_files(qrels_file, run_file):\n",
    "    return read_qrels_file(qrels_file), read_run_file(run_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(relevant, retrieved, k):\n",
    "    if k==0:\n",
    "        return 1\n",
    "    elif (k > 0) and (type(k) == int):\n",
    "        tp_at_k = [doc for doc in retrieved[:k] if doc in relevant]\n",
    "        return len(tp_at_k) / len(retrieved[:k])\n",
    "    else:\n",
    "        print(\"k has a wrong value\")\n",
    "        \n",
    "def interpolated_precision_at_recall_X (relevant, retrieved, X):\n",
    "    precisions = []\n",
    "    for i in range(len(retrieved) + 1):\n",
    "        tp_at_i = [doc for doc in retrieved[:i] if doc in relevant]\n",
    "        recall_at_i = len(tp_at_i) / len(relevant)\n",
    "        if recall_at_i >= X:\n",
    "            precision_at_i = precision_at_k(relevant, retrieved, i)\n",
    "            precisions.append(precision_at_i)\n",
    "    if (len(precisions) == 0):\n",
    "        return 0\n",
    "    return max(precisions)\n",
    "\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "    eleven_points = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    precisions = []\n",
    "    for recall in eleven_points:\n",
    "        precisions.append(interpolated_precision_at_recall_X(relevant, retrieved, recall))\n",
    "    return sum(precisions) / len(precisions)\n",
    "\n",
    "\n",
    "def mean_metric(measure, all_relevant, all_retrieved):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for qid in all_relevant:\n",
    "        relevant  = all_relevant[qid]\n",
    "        retrieved = all_retrieved.get(qid, [])\n",
    "        value = measure(relevant, retrieved)\n",
    "        total += value\n",
    "        count += 1\n",
    "    return \"mean \" + measure.__name__, total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trec_run(es, topics_file_name, run_file_name, run_name=\"test\"):\n",
    "    with open(run_file_name, 'w') as run_file:\n",
    "        with open(topics_file_name, 'r') as test_queries:\n",
    "            for line in test_queries:\n",
    "                (qid, query) = line.strip().split('\\t')\n",
    "                s = Search(using=es, index='genomics')[:10000].query(\"multi_match\", query=query)\n",
    "                response = s.execute()\n",
    "                s = \"\"\n",
    "                for rank, hit in enumerate(response.to_dict()['hits']['hits']):\n",
    "                    line = [str(qid), \"Q0\", str(hit['_source']['PMID']), str(rank + 1), str(hit['_score']), run_name]\n",
    "                    run_file.write(\" \".join(line))\n",
    "                    run_file.write(\"\\n\")\n",
    "\n",
    "                    \n",
    "def trec_eval(qrels_file, run_file):\n",
    "    def precision_at_1(rel, ret): return precision_at_k(rel, ret, k=1)\n",
    "    def precision_at_5(rel, ret): return precision_at_k(rel, ret, k=5)\n",
    "    def precision_at_10(rel, ret): return precision_at_k(rel, ret, k=10)\n",
    "    def precision_at_50(rel, ret): return precision_at_k(rel, ret, k=50)\n",
    "    def precision_at_100(rel, ret): return precision_at_k(rel, ret, k=100)\n",
    "    def precision_at_recall_00(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.0)\n",
    "    def precision_at_recall_01(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.1)\n",
    "    def precision_at_recall_02(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.2)\n",
    "    def precision_at_recall_03(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.3)\n",
    "    def precision_at_recall_04(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.4)\n",
    "    def precision_at_recall_05(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.5)\n",
    "    def precision_at_recall_06(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.6)\n",
    "    def precision_at_recall_07(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.7)\n",
    "    def precision_at_recall_08(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.8)\n",
    "    def precision_at_recall_09(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.9)\n",
    "    def precision_at_recall_10(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=1.0)\n",
    "    def average_precision(rel, ret):\n",
    "        temp_scores = [x for (_, x) in metrics_scores]\n",
    "        return sum(temp_scores) / len(temp_scores)\n",
    "\n",
    "    (all_relevant, all_retrieved) = read_eval_files(qrels_file, run_file)\n",
    "    \n",
    "    unknown_qids = set(all_retrieved.keys()).difference(all_relevant.keys())\n",
    "    if len(unknown_qids) > 0:\n",
    "        raise ValueError(\"Unknown qids in run: {}\".format(sorted(list(unknown_qids))))\n",
    "\n",
    "    metrics = [\n",
    "        precision_at_recall_00,\n",
    "        precision_at_recall_01,\n",
    "        precision_at_recall_02,\n",
    "        precision_at_recall_03,\n",
    "        precision_at_recall_04,\n",
    "        precision_at_recall_05,\n",
    "        precision_at_recall_06,\n",
    "        precision_at_recall_07,\n",
    "        precision_at_recall_08,\n",
    "        precision_at_recall_09,\n",
    "        precision_at_recall_10,\n",
    "        average_precision,\n",
    "        precision_at_1,\n",
    "        precision_at_5,\n",
    "        precision_at_10,\n",
    "        precision_at_50,\n",
    "        precision_at_100\n",
    "    ]   \n",
    "    \n",
    "    metrics_scores = []\n",
    "    for metric in metrics:\n",
    "        metrics_scores.append(mean_metric(metric, all_relevant, all_retrieved))\n",
    "\n",
    "    return metrics_scores\n",
    "\n",
    "\n",
    "def print_trec_eval(qrels_file, run_file):\n",
    "    results = trec_eval(qrels_file, run_file)\n",
    "    print(\"Results for {}\".format(run_file))\n",
    "    for (metric, score) in results:\n",
    "        print(\"{:<30} {:.4}\".format(metric, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuilt BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuilt_bm25 = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\" : 1,\n",
    "        \"index\" : {\n",
    "            \"similarity\" : {\n",
    "                \"my_similarity\" : {\n",
    "                    \"type\": \"BM25\",\n",
    "                    \"k1\": 1.2,\n",
    "                    \"b\": 0.75\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords\": \"_english_\" \n",
    "                },\n",
    "                \"english_stemmer\": {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"language\": \"english\"\n",
    "                },\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"my_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\",\n",
    "                        \"english_stemmer\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"AB\": {\n",
    "                \"type\": \"text\",\n",
    "                \"copy_to\": \"all\"\n",
    "            },\n",
    "            \"TI\": {\n",
    "                \"type\": \"text\",\n",
    "                \"copy_to\": \"all\"\n",
    "            },\n",
    "            \"all\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"my_similarity\",\n",
    "                \"analyzer\":\"my_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_documents(es, 'data/trec-medline.json', 'genomics', body=rebuilt_bm25)\n",
    "make_trec_run(es, 'data/training-queries-simple.txt', 'project_rebuilt_bm25.run', run_name='project01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\" : 1,\n",
    "        \"index\" : {\n",
    "            \"similarity\" : {\n",
    "                \"my_similarity\" : {\n",
    "                    \"type\": \"BM25\",\n",
    "                    \"k1\": 1.2,\n",
    "                    \"b\": 0.75\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"AB\": {\n",
    "                \"type\": \"text\",\n",
    "                \"copy_to\": \"all\"\n",
    "            },\n",
    "            \"TI\": {\n",
    "                \"type\": \"text\",\n",
    "                \"copy_to\": \"all\"\n",
    "            },\n",
    "            \"all\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"my_similarity\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_documents(es, 'data/trec-medline.json', 'genomics', body=bm25)\n",
    "make_trec_run(es, 'data/training-queries-simple.txt', 'project_bm25.run', run_name='project01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let model 1 is rebuilt bm25, model 2 is simple bm25:\n",
      "n= 17  t= 3.2491  p= 0.00251548\n",
      "significant，model 1 better\n"
     ]
    }
   ],
   "source": [
    "def is_signiﬁcance(array_model1, array_model2):\n",
    "    if len(array_model1) != 0 and len(array_model2) != 0:\n",
    "        alpha = 0.05\n",
    "        t, p_2tailed = stats.ttest_rel(array_model1, array_model2)\n",
    "        p_1tailed = p_2tailed / 2\n",
    "        print(\"n=\", len(array_model1), \" t=\", format(t, '0.4f'), \" p=\", format(p_1tailed, '0.8f'))\n",
    "        if p_1tailed < alpha and t < 0:\n",
    "            print(\"significant，model 2 better\")\n",
    "        elif p_1tailed < alpha and t > 0:\n",
    "            print(\"significant，model 1 better\")\n",
    "        elif p_1tailed >= alpha and t < 0:\n",
    "            print(\"not significant, model 2 seems better\")\n",
    "        else:\n",
    "            print(\"not significant, model 1 seems better\")\n",
    "\n",
    "            \n",
    "def get_scores(results):\n",
    "    return [x for (_, x) in results]\n",
    "\n",
    "\n",
    "if not os.path.exists(\"scores.json\"):\n",
    "    # save the results of trec_eval because it's really time consuming.\n",
    "    project_rebuilt_bm25_results = trec_eval('data/training-qrels.txt', 'project_rebuilt_bm25.run')\n",
    "    project_bm25_results = trec_eval('data/training-qrels.txt', 'project_bm25.run')\n",
    "    \n",
    "    project_rebuilt_bm25_scores = get_scores(project_rebuilt_bm25_results)\n",
    "    project_bm25_scores = get_scores(project_bm25_results)\n",
    "    scores = {\n",
    "        \"rebuilt_bm25\": project_rebuilt_bm25_scores,\n",
    "        \"bm25\": project_bm25_scores,\n",
    "    }\n",
    "    jsObj = json.dumps(scores)\n",
    "    with open(\"scores.json\", \"w\") as f:\n",
    "        f.write(jsObj)\n",
    "else:\n",
    "    with open('scores.json', 'r') as f:\n",
    "        scores = json.load(f)\n",
    "        project_rebuilt_bm25_scores = scores['rebuilt_bm25']\n",
    "        project_bm25_scores = scores['bm25']\n",
    "        \n",
    "print(\"Let model 1 is rebuilt bm25, model 2 is simple bm25:\")\n",
    "is_signiﬁcance(project_rebuilt_bm25_scores, project_bm25_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
